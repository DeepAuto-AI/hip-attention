{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ainl/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9333aae670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.generation import GenerationConfig\n",
    "from src.models.qwen.modeling_qwen import QWenLMHeadModel\n",
    "import torch\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-VL:\n",
      "- tokenization_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618609cd1e4f4d77b0bd01b353274853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa0e14cd628468ba592dffc1850a681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = QWenLMHeadModel.from_pretrained(\n",
    "    \"Qwen/Qwen-VL\", \n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        llm_int8_skip_modules=['visual']\n",
    "    ),\n",
    "    fp16=True,\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ainl/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ainl/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picture 1: <img>notebook/mmmu_image.png</img>\n",
      "\n",
      "tensor([[ 24669,    220,     16,     25,    220, 151857,    110,    111,    116,\n",
      "            101,     98,    111,    111,    107,     47,    109,    109,    109,\n",
      "            117,     95,    105,    109,     97,    103,    101,     46,    112,\n",
      "            110,    103, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859, 151859,\n",
      "         151859, 151858,    198]], device='cuda:0')\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask']) torch.Size([1, 264]) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 15:59:07.565884: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-06 15:59:07.565941: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-06 15:59:07.565945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 412])\n",
      "Picture 1: <img>notebook/mmmu_image.png</img>\n",
      "Company A Company B Company C Company D 1 Revenues ? $1,480,500 $103,950 $1,054,116 2 Expenses $455,490 1,518,300 78,120 ? 3 Gains 0 ? 4,725 8,505 4 Losses 32,760 0 5,670 39,312 5 Net Income or (Loss) 32,130 39,690 ? ? (58,275)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "query = tokenizer.from_list_format([\n",
    "    {'image': 'notebook/mmmu_image.png'},\n",
    "    # {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\n",
    "    # {'text': 'Each of the following situations relates to a different company. For company B, find the missing amounts. \\n\\n A: $63,020 \\n\\n B: $58,410 \\n\\n C: $71,320 \\n\\n D: $77,490 \\n\\n Answer:'},\n",
    "])\n",
    "print(query)\n",
    "inputs = tokenizer(query, return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "print(inputs.keys(), inputs.input_ids.shape, print(inputs.input_ids))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model.generate(**inputs, do_sample=False)\n",
    "\n",
    "print(pred.shape)\n",
    "\n",
    "response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 264, 151936])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('MMMU/MMMU', 'Accounting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$6', '$7', '$8', '$9']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(ds['validation'][0]['options'].replace(\"'\", '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"PngImageFile\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# {'text': 'Each of the following situations relates to a different company. For company B, find the missing amounts. \\n\\n A: $63,020 \\n\\n B: $58,410 \\n\\n C: $71,320 \\n\\n D: $77,490 \\n\\n Answer:'},\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m query\n",
      "File \u001b[0;32m/d1/heejun/.cache/huggingface/transformers/modules/transformers_modules/Qwen/Qwen-VL/0547ed36a86561e2e42fecec8fd0c4f6953e33c4/tokenization_qwen.py:406\u001b[0m, in \u001b[0;36mQWenTokenizer.from_list_format\u001b[0;34m(self, list_format)\u001b[0m\n\u001b[1;32m    404\u001b[0m     num_images \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    405\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPicture \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 406\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_start_tag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mele\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_end_tag\n\u001b[1;32m    407\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ele:\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"PngImageFile\") to str"
     ]
    }
   ],
   "source": [
    "query = tokenizer.from_list_format([\n",
    "    {'image': ds['validation'][0]['image_1']},\n",
    "    # {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\n",
    "    # {'text': 'Each of the following situations relates to a different company. For company B, find the missing amounts. \\n\\n A: $63,020 \\n\\n B: $58,410 \\n\\n C: $71,320 \\n\\n D: $77,490 \\n\\n Answer:'},\n",
    "])\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9044b65c094a67b5d6f7ce1a29bc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ecb82f74c849c590b56dc61bd04506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1b92f711c74159b76fcd66c60282ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe778c0db9f4a25b4068faae43cc574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'options', 'explanation', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5', 'image_6', 'image_7', 'img_type', 'answer', 'topic_difficulty', 'question_type', 'subfield'],\n",
       "        num_rows: 35\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'options', 'explanation', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5', 'image_6', 'image_7', 'img_type', 'answer', 'topic_difficulty', 'question_type', 'subfield'],\n",
       "        num_rows: 380\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_dataset('./cache/MMMU/Accounting', 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ainl/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.0417e-07)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One pathogen',\n",
       " 'Two pathogens',\n",
       " 'Three pathogens',\n",
       " 'There is no pathogen involved',\n",
       " 'I don\"t know and I don\"t want to guess']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads('[\"One pathogen\", \"Two pathogens\", \"Three pathogens\", \"There is no pathogen involved\", \"I don\\\\\"t know and I don\\\\\"t want to guess\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(\"['a']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
